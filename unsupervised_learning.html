

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>4. Unsupervised learning: seeking representations of the data &mdash; scikit-learn tutorial v0.7+ documentation</title>
    <link rel="stylesheet" href="_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     '0.7+',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="shortcut icon" href="_static/favicon.ico"/>
    <link rel="top" title="scikit-learn tutorial v0.7+ documentation" href="index.html" />
    <link rel="next" title="5. Putting it all together" href="putting_together.html" />
    <link rel="prev" title="3. Model selection: choosing estimators and their parameters" href="model_selection.html" /> 
  </head>
  <body>
    <div class="header-wrapper">
      <div class="header">
          <p class="logo"><a href="index.html">
            <img src="_static/scikit-learn-logo-small.png" alt="Logo"/>
          </a>
          </p><div class="navbar">
          <ul>
          <li><a href="model_selection.html" title="3. Model selection: choosing estimators and their parameters"
             accesskey="P">previous</a></li> |
          <li><a href="putting_together.html" title="5. Putting it all together"
             accesskey="N">next</a></li>
       </ul>

	&nbsp;
	<ul>
	<li><a href="index.html">Index</a>
	</li>
	</ul>

       </div> <!-- end navbar --></div>
    </div>

    <div class="content-wrapper">

    <!-- <div id="blue_tile"></div> -->

      <div class="content">
            
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="unsupervised-learning-seeking-representations-of-the-data">
<h1>4. Unsupervised learning: seeking representations of the data<a class="headerlink" href="#unsupervised-learning-seeking-representations-of-the-data" title="Permalink to this headline">¶</a></h1>
<div class="section" id="clustering-grouping-observations-together">
<h2>4.1. Clustering: grouping observations together<a class="headerlink" href="#clustering-grouping-observations-together" title="Permalink to this headline">¶</a></h2>
<div class="topic">
<p class="topic-title first">The problem solved in clustering</p>
<p>Given the iris dataset, if we knew that there were 3 types of iris, but
did not have access to a taxonomist to label them: we could try a
<strong>clustering task</strong>: split the observations in well-separated group
called <em>clusters</em>.</p>
</div>
<div class="section" id="k-means-clustering">
<h3>4.1.1. K-means clustering<a class="headerlink" href="#k-means-clustering" title="Permalink to this headline">¶</a></h3>
<p>Note that their exists many different clustering criteria and associated
algorithm. The simplest clustering algorithm is the k-means.</p>
<a class="reference internal image-reference" href="_images/k_means_iris_3.png"><img align="right" alt="_images/k_means_iris_3.png" class="align-right" src="_images/k_means_iris_3.png" style="width: 280.0px; height: 210.0px;" /></a>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scikits.learn</span> <span class="kn">import</span> <span class="n">cluster</span><span class="p">,</span> <span class="n">datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_iris</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_iris</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">k_means</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">KMeans</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k_means</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_iris</span><span class="p">)</span> 
<span class="go">KMeans(verbose=0, k=3, max_iter=300, init=&#39;k-means++&#39;,...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">k_means</span><span class="o">.</span><span class="n">labels_</span><span class="p">[::</span><span class="mi">10</span><span class="p">]</span>
<span class="go">[1 1 1 1 1 0 0 0 0 0 2 2 2 2 2]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">y_iris</span><span class="p">[::</span><span class="mi">10</span><span class="p">]</span>
<span class="go">[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2]</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p>There is absolutely no garanty of recovering a ground truth. First
choosing the right number of clusters is hard. Second, the algorithm
is sensitive to initialization, and can fall in local minima,
although in the <cite>scikits.learn</cite> we play many tricks to mitigate this
issue.</p>
<table border="1" class="centered docutils">
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<tbody valign="top">
<tr><td><a class="reference internal" href="_images/k_means_iris_bad_init.png"><img alt="k_means_iris_bad_init" src="_images/k_means_iris_bad_init.png" style="width: 252.0px; height: 189.0px;" /></a></td>
<td><a class="reference internal" href="_images/k_means_iris_8.png"><img alt="k_means_iris_8" src="_images/k_means_iris_8.png" style="width: 252.0px; height: 189.0px;" /></a></td>
<td><a class="reference internal" href="_images/cluster_iris_truth.png"><img alt="cluster_iris_truth" src="_images/cluster_iris_truth.png" style="width: 252.0px; height: 189.0px;" /></a></td>
</tr>
<tr><td><strong>Bad initialization</strong></td>
<td><strong>8 clusters</strong></td>
<td><strong>Ground truth</strong></td>
</tr>
</tbody>
</table>
<p class="last"><strong>Don&#8217;t over-interpret clustering results</strong></p>
</div>
<div class="topic">
<p class="topic-title first"><strong>Application example: vector quantization</strong></p>
<p>Clustering in general and KMeans in particular, can be seen as a way
of choosing a small number of examplars to compress the information,
a problem sometimes known as vector quantization. For instance, this
can be used to posterize an image:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">scipy</span> <span class="kn">as</span> <span class="nn">sp</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lena</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">lena</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">lena</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="c"># We need an (n_sample, n_feature) array</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k_means</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">KMeans</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k_means</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">values</span> <span class="o">=</span> <span class="n">k_means</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">k_means</span><span class="o">.</span><span class="n">labels_</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lena_compressed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">choose</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lena_compressed</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">lena</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<table border="1" class="centered docutils">
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<tbody valign="top">
<tr><td><a class="reference internal" href="_images/lena.png"><img alt="lena" src="_images/lena.png" style="width: 153.6px; height: 153.6px;" /></a></td>
<td><a class="reference internal" href="_images/lena_compressed.png"><img alt="lena_compressed" src="_images/lena_compressed.png" style="width: 153.6px; height: 153.6px;" /></a></td>
<td><a class="reference internal" href="_images/lena_regular.png"><img alt="lena_regular" src="_images/lena_regular.png" style="width: 153.6px; height: 153.6px;" /></a></td>
<td><a class="reference internal" href="_images/lena_histogram.png"><img alt="lena_histogram" src="_images/lena_histogram.png" style="width: 180.0px; height: 132.0px;" /></a></td>
</tr>
<tr><td>Raw image</td>
<td>K-means quantization</td>
<td>Equal bins</td>
<td>Image histogram</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="hierarchical-clustering-ward">
<h3>4.1.2. Hierarchical clustering: Ward<a class="headerlink" href="#hierarchical-clustering-ward" title="Permalink to this headline">¶</a></h3>
<p>For estimating a large number of clusters, top-down approaches are both
statisticaly ill-posed, and slow. Hierarchical clustering is a bottom-up
approach that merges successively observations together and is
particularly useful when the clusters of interest are made of only a few
observations. <em>Ward</em> clustering minimizes a criterion similar to k-means
in a bottom-up approach. When the number of clusters is large, it is much
more computationally efficient than k-means.</p>
<div class="section" id="connectivity-constrained-clustering">
<h4>4.1.2.1. Connectivity-constrained clustering<a class="headerlink" href="#connectivity-constrained-clustering" title="Permalink to this headline">¶</a></h4>
<p>With Ward clustering, it is possible to specify which samples can be
clustered together by giving a connectivity graph. Graphs in the scikit
are represented by their adjacency matrix. Often a sparse matrix is used.
This can be useful for instance to retrieve connect regions when
clustering an image:</p>
<a class="reference internal image-reference" href="_images/lena_ward.png"><img align="right" alt="_images/lena_ward.png" class="align-right" src="_images/lena_ward.png" style="width: 179.2px; height: 179.2px;" /></a>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="c"># Downsample the image by a factor of 4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lena</span> <span class="o">=</span> <span class="n">lena</span><span class="p">[::</span><span class="mi">2</span><span class="p">,</span> <span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">lena</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">,</span> <span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">lena</span><span class="p">[::</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">lena</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">lena</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c"># the structure of the data: pixels connected to their neighbors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scikits.learn.feature_extraction.image</span> <span class="kn">import</span> <span class="n">grid_to_graph</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">connectivity</span> <span class="o">=</span> <span class="n">grid_to_graph</span><span class="p">(</span><span class="o">*</span><span class="n">lena</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">ward</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">Ward</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ward</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">connectivity</span><span class="o">=</span><span class="n">connectivity</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ward</span><span class="o">.</span><span class="n">labels_</span><span class="p">,</span> <span class="n">lena</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="feature-agglomeration">
<h4>4.1.2.2. Feature agglomeration<a class="headerlink" href="#feature-agglomeration" title="Permalink to this headline">¶</a></h4>
<p>We have seen that sparsity could be used to mitigate the curse of
dimensionality, <em>i.e</em> the insufficience of observations compared to the
number of features. Another approach is to merge together similar
features: <strong>feature agglomeration</strong>. This approach can be implementing by
clustering in the feature direction, in other words clustering the
transposed data.</p>
<a class="reference internal image-reference" href="_images/digits_agglo.png"><img align="right" alt="_images/digits_agglo.png" class="align-right" src="_images/digits_agglo.png" style="width: 228.0px; height: 199.5px;" /></a>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">digits</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_digits</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">images</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">images</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">images</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">connectivity</span> <span class="o">=</span> <span class="n">grid_to_graph</span><span class="p">(</span><span class="o">*</span><span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">agglo</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">WardAgglomeration</span><span class="p">(</span><span class="n">connectivity</span><span class="o">=</span><span class="n">connectivity</span><span class="p">,</span>
<span class="gp">... </span>                                  <span class="n">n_clusters</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">agglo</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_reduced</span> <span class="o">=</span> <span class="n">agglo</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">X_approx</span> <span class="o">=</span> <span class="n">agglo</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X_reduced</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">images_approx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_restored</span><span class="p">,</span> <span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first"><cite>transform</cite> and <cite>inverse_transform</cite> methods</p>
<p>Some estimators expose a <cite>transform</cite> method, for instance to reduce
the dimensionality of the dataset.</p>
</div>
</div>
</div>
</div>
<div class="section" id="decompositions-from-a-signal-to-components-and-loadings">
<h2>4.2. Decompositions: from a signal to components and loadings<a class="headerlink" href="#decompositions-from-a-signal-to-components-and-loadings" title="Permalink to this headline">¶</a></h2>
<div class="topic">
<p class="topic-title first"><strong>Components and loadings</strong></p>
<p>If X is our multivariate data, the problem that we are trying to solve
is to rewrite it on a different observation basis: we want to learn
loadings L and a set of components C such that <em>X = L C</em>.
Different criteria exist to choose the components</p>
</div>
<div class="section" id="principal-component-analysis-pca">
<h3>4.2.1. Principal component analysis: PCA<a class="headerlink" href="#principal-component-analysis-pca" title="Permalink to this headline">¶</a></h3>
<p>Principal component analysis select the successive components that
explain the maximum variance in the signal.</p>
<p class="centered"><a class="reference internal" href="_images/pca_3d_axis.jpg"><img alt="pca_3d_axis" src="_images/pca_3d_axis.jpg" style="width: 280.0px; height: 224.0px;" /></a> <a class="reference internal" href="_images/pca_3d_aligned.jpg"><img alt="pca_3d_aligned" src="_images/pca_3d_aligned.jpg" style="width: 280.0px; height: 224.0px;" /></a></p>
<p>The point cloud spanned by the observations above is very flat in one
direction: one of the 3 univariate features can almost be exactly
computed using the 2 other. PCA finds the directions in which the data is
not <em>flat</em></p>
<p>When used to <em>transform</em> data, PCA can reduce the dimensionality of the
data by projecting on a principal subspace.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="c"># Create a signal with only 2 useful dimensions</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x3</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">+</span> <span class="n">x2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scikits.learn</span> <span class="kn">import</span> <span class="n">decomposition</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span> <span class="o">=</span> <span class="n">decomposition</span><span class="o">.</span><span class="n">PCA</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_</span>
<span class="go">[  2.77227227e+00,   1.14228495e+00,   2.66364138e-32]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">Only</span> <span class="n">the</span> <span class="mi">2</span> <span class="n">first</span> <span class="n">components</span> <span class="n">are</span> <span class="n">useful</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_reduced</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_reduced</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(100, 2)</span>
</pre></div>
</div>
</div>
<div class="section" id="indenpendant-component-analysis-ica">
<h3>4.2.2. Indenpendant Component Analysis: ICA<a class="headerlink" href="#indenpendant-component-analysis-ica" title="Permalink to this headline">¶</a></h3>
<p>ICA selects components so that the distribution of their loadings carries
a maximum amount of independant information. It is able to recover
<strong>non-Gaussian</strong> independant signals:</p>
<a class="reference internal image-reference" href="_images/plot_ica_blind_source_separation_1.png"><div align="center" class="align-center"><img alt="_images/plot_ica_blind_source_separation_1.png" class="align-center" src="_images/plot_ica_blind_source_separation_1.png" style="width: 560.0px; height: 420.0px;" /></div>
</a>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="c"># Generate sample data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">time</span><span class="p">)</span>  <span class="c"># Signal 1 : sinusoidal signal</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">3</span> <span class="o">*</span> <span class="n">time</span><span class="p">))</span>  <span class="c"># Signal 2 : square signal</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">S</span> <span class="o">+=</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">S</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c"># Add noise</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">S</span> <span class="o">/=</span> <span class="n">S</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c"># Standardize data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c"># Mix data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>  <span class="c"># Mixing matrix</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>  <span class="c"># Generate observations</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c"># Compute ICA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ica</span> <span class="o">=</span> <span class="n">decomposition</span><span class="o">.</span><span class="n">FastICA</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">S_</span> <span class="o">=</span> <span class="n">ica</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  <span class="c"># Get the estimated sources</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">A_</span> <span class="o">=</span> <span class="n">ica</span><span class="o">.</span><span class="n">get_mixing_matrix</span><span class="p">()</span>  <span class="c"># Get estimated mixing matrix</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">S_</span><span class="p">,</span> <span class="n">A_</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
<span class="go">True</span>
</pre></div>
</div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
        <div class="clearer"></div>
      </div>
    <div class="navbar" style="float: left; margin-left: 15pt;">
    <ul>
    <li><a href="model_selection.html" title="3. Model selection: choosing estimators and their parameters"
	>previous</a></li> |
    <li><a href="putting_together.html" title="5. Putting it all together"
	>next</a></li>
    </ul>
    </div>

    </div>

    <div class="footer">
        <p style="text-align: center">This tutorial is for scikit-learn
        version 0.7+<p>
        &copy; 2010–2011, scikits.learn developers (BSD License).
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.0. Design by <a href="http://webylimonada.com">Web y Limonada</a>.
    </div>
  </body>
</html>